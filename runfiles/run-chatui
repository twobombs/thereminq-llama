#!/bin/bash

# X-run script
rm /tmp/.X0-lock
Xvfb :0 -screen 0 1920x1080x24+32 -ac &
export DISPLAY=:0&&startxfce4 &
sleep 10
x0vncserver -display :0 -passwordfile ~/.vnc/passwd &
/usr/share/novnc/utils/launch.sh --vnc localhost:5900 &
service xrdp start && xrdp &

export DISPLAY=:0&&xterm -e "xset s off" &
export DISPLAY=:0&&xterm -e "neofetch && tail -f /dev/null" &

# qrack_cl_precompile
/qrack/_build/qrack_cl_precompile

# link to default model and start webui + API
ln -s /Llama-3-DARE-8B.IQ3_M.gguf /text-generation-webui/models/Llama-3-DARE-8B.IQ3_M.gguf
cd  /text-generation-webui && python3 server.py --listen --share --api --trust-remote-code --gradio-auth thereminq:00000000 &

#fetch model
rm Llama-3-DARE-8B.IQ3_M.gguf && wget https://huggingface.co/mradermacher/Llama-3-DARE-8B-GGUF/resolve/main/Llama-3-DARE-8B.IQ3_M.gguf &

# start open interpreter UI 
cd /OpenInterpreterUI && streamlit run app.py --server.port 8501 &

# clean desktop & info
export DISPLAY=:0&&xterm -e "xset s off" &
export DISPLAY=:0&&xterm -e "neofetch && tail -f /dev/null" &
export DISPLAY=:0&&xterm -e "./LM_Studio-0.2.27.AppImage" &

# when fail do not stop container ( debug )
tail -f /dev/null
