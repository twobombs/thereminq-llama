#!/bin/bash

# X-run script
rm /tmp/.X0-lock
Xvfb :0 -screen 0 1920x1080x24+32 -ac &
export DISPLAY=:0&&startxfce4 &
sleep 10
x0vncserver -display :0 -passwordfile ~/.vnc/passwd &
/usr/share/novnc/utils/launch.sh --vnc localhost:5900 &
service xrdp start && xrdp &

export DISPLAY=:0&&xterm -e "xset s off" &
export DISPLAY=:0&&xterm -e "neofetch && tail -f /dev/null" &

# qrack_cl_precompile
/qrack/_build/qrack_cl_precompile

# start ollama with llama 3
# start open-interpreter with webui
OLLAMA_HOST=0.0.0.0 ollama serve &
sleep 5

cd /OpenInterpreterUI && streamlit run app.py --server.port 8501 &

# start typingmind
cd TypingMind && npm run start &

export DISPLAY=:0 && google-chrome-stable --disable-fre --no-default-browser-check --no-first-run --no-sandbox --password-store=basic http://localhost:3000 &
export DISPLAY=:0 && falkon --disable-fre --no-default-browser-check --no-first-run --no-sandbox --password-store=basic http://localhost:3000 &
export DISPLAY=:0&&exo-open --launch TerminalEmulator &

# start open-interpreter with local api ( CLI input when the container is run on tty - not forked )
ollama pull llama3 && interpreter -y --model ollama/llama3

# when fail do not stop container ( debug )
tail -f /dev/null
