#!/bin/bash

# qrack_cl_precompile
/qrack/_build/qrack_cl_precompile

#start ollama pull llama 3
# start open-interpreter with webui
OLLAMA_HOST=0.0.0.0 ollama serve &
sleep 3
ollama pull llama3 && cd /OpenInterpreterUI && streamlit run app.py --server.port 8501 &

# start open-interpreter with openai local api ( CLI input when the container is run on tty - not forked )
interpreter -y  --model ollama/llama3 --api_base=http://localhost:11434

# when fail do not stop container ( debug )
tail -f /dev/null
