FROM FROM twobombs/thereminq-bonsai:qrack-precooked

# fetch dependancies
RUN export DEBIAN_FRONTEND=noninteractive && apt update && apt install -y git python3-pip mc sudo wget libopenblas-dev libclblast-dev && apt clean all

#fetch llama.cpp
RUN git clone --depth=1 https://github.com/ggerganov/llama.cpp
RUN cp -r llama.cpp llama-openblas
RUN cp -r llama.cpp llama-cublas
RUN cp -r llama.cpp llama-clblast

# build all versions
RUN cd llama.cpp && make -j
RUN cd llama-openblas && cmake . && make LLAMA_OPENBLAS=1 all
RUN cd llama-cublas && cmake -DGGML_CUBLAS=ON . && make LLAMA_CUBLAS=1 all
RUN cd llama-clblast && cmake -DGGML_CLBLAST=ON . && make LLAMA_CLBLAST=1 all

RUN whet https://huggingface.co/TheBloke/WizardMath-7B-V1.1-GGUF/resolve/main/wizardmath-7b-v1.1.Q2_K.gguf

RUN ln -s wizardmath-7b-v1.1.Q2_K.gguf /llama.cpp/models/wizardmath-7b-v1.1.Q2_K.gguf
RUN ln -s wizardmath-7b-v1.1.Q2_K.gguf /llama-cublas/models/wizardmath-7b-v1.1.Q2_K.gguf
RUN ln -s wizardmath-7b-v1.1.Q2_K.gguf /llama-clblast/models/wizardmath-7b-v1.1.Q2_K.gguf
RUN ln -s wizardmath-7b-v1.1.Q2_K.gguf /llama-openblas/models/wizardmath-7b-v1.1.Q2_K.gguf

# test build
# RUN cd llama.cpp && ./main -m /llama.cpp/models/wizardmath-7b-v1.1.Q2_K.gguf -p "predict the next median using the following numbers : 10 20 30" -n 512
