FROM twobombs/thereminq-llama

RUN git clone https://github.com/twobombs/thereminq-llama

#fetch llama.cpp
RUN git clone --depth=1 https://github.com/ggerganov/llama.cpp
RUN cp -r llama.cpp llama-openblas
RUN cp -r llama.cpp llama.hip
RUN cp -r llama.cpp llama-vulkan

#fetch local deep research agency
RUN git clone --depth=1 https://github.com/LearningCircuit/local-deep-research

#setup
RUN pip install --break-system-packages --upgrade setuptools && pip cache purge
RUN apt update && apt install -y libvulkan-dev glslc libcurl4-gnutls-dev cmake build-essential && apt clean all

# build versions via cmake
RUN cd llama.cpp && cmake -B build && cmake --build build --config Release -j $(grep -c ^processor /proc/cpuinfo)
RUN cd llama-openblas && cmake -B build -DGGML_BLAS_VENDOR=OpenBLAS && cmake --build build --config Release -j $(grep -c ^processor /proc/cpuinfo)
RUN cd llama-vulkan && cmake -B build -DGGML_VULKAN=1 && cmake --build build --config Release -j $(grep -c ^processor /proc/cpuinfo)

# fetch nanochat for factory framework
RUN pip install --break-system-packages huggingface_hub[cli]
RUN git clone https://github.com/karpathy/nanochat.git

# RUN hf download karpathy/nanochat-d34 --local-dir nanochat-d34
# install llama.cpp conversion dependancies
# RUN pip3 install --break-system-packages --ignore-installed torch sentencepiece transformers && pip cache purge
# install mergekit for llm merging
# RUN pip3 install --break-system-packages mergekit && pip cache purge

# test build
# RUN cd llama.cpp && ./main -m /llama.cpp/models/Meta-Llama-3.1-8B-q4_k_s.gguf -p "make a proof read of the negative energy spike when teleporting a qubit according to ER=EPR and devise a strategy to execute the thesis on a quantum computer that shows that ER=EPR is an example of quantum gravity" -n 512

ENTRYPOINT /root/run
